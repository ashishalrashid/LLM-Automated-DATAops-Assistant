{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_token = os.getenv(\"AIPROXY_TOKEN\")\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "from dateutil.parser import parse\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import easyocr\n",
    "# import beautifulsoup4\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "}\n",
    "url=\"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_relative_path(path: str) -> str:\n",
    "    print(\"I have seen your kind time and time again\")\n",
    "    return path.lstrip(\"/\")  # If absolute local path, remove leading slash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate code\n",
    "\n",
    "def execute_generated_code(code, arguments=None):\n",
    "    # Save the provided code to a temporary file.\n",
    "    script_path = \"generated_script.py\"\n",
    "    with open(script_path, \"w\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "    if arguments:\n",
    "        param_str = \" \".join(f'--{k} {v}' for k, v in arguments.items())\n",
    "        command = f\"python {script_path} {param_str}\"\n",
    "    else:\n",
    "        command = f\"python {script_path}\"\n",
    "\n",
    "    # Execute the script and capture output.\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout if result.stdout else result.stderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def execute_generated_code(code, arguments=None):\n",
    "    # Ensure that Python uses UTF-8 for standard I/O.\n",
    "    os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "    \n",
    "    # Save the provided code to a temporary file using UTF-8 encoding.\n",
    "    script_path = \"generated_script.py\"\n",
    "    with open(script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(code)\n",
    "\n",
    "    # Prepare the command line arguments if provided.\n",
    "    if arguments:\n",
    "        param_str = \" \".join(f'--{k} {v}' for k, v in arguments.items())\n",
    "        command = f\"python {script_path} {param_str}\"\n",
    "    else:\n",
    "        command = f\"python {script_path}\"\n",
    "\n",
    "    # Execute the script and capture output, forcing UTF-8 encoding.\n",
    "    result = subprocess.run(\n",
    "        command, \n",
    "        shell=True, \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    \n",
    "    return result.stdout if result.stdout else result.stderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and retive files\n",
    "\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import subprocess\n",
    "\n",
    "def run_download_from_script(url, user_email=None, *args):\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    \n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    cmd = [\"uv\",\"run\", filename]\n",
    "    if user_email is not None:\n",
    "        cmd.append(user_email)\n",
    "    cmd.extend(args)\n",
    "    print(cmd)\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "\n",
    "#run_remote_script(\"https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py\", \"22f1001551@ds.study.iitm.ac.in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run npx package\n",
    "\n",
    "def run_npx_package(package, arg=None):\n",
    "    npx = \"npx.cmd\" if os.name == \"nt\" else \"npx\"\n",
    "    cmd = [npx, package]\n",
    "    if arg:\n",
    "        cmd.extend(shlex.split(arg))\n",
    "    print(\"Running command:\", \" \".join(cmd))\n",
    "    result = subprocess.run(cmd)\n",
    "    if result.returncode:\n",
    "        sys.exit(result.returncode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_day_occurrences\n",
    "\n",
    "\n",
    "def count_day_occurrences(input_path, output_path, day):\n",
    "    input_path = make_relative_path(input_path)\n",
    "    output_path = make_relative_path(output_path)\n",
    "    \n",
    "    day = day.lower()\n",
    "    day_map = {\n",
    "        \"monday\": 0,\n",
    "        \"tuesday\": 1,\n",
    "        \"wednesday\": 2,\n",
    "        \"thursday\": 3,\n",
    "        \"friday\": 4,\n",
    "        \"saturday\": 5,\n",
    "        \"sunday\": 6\n",
    "    }\n",
    "    \n",
    "\n",
    "    target_weekday = day_map[day]\n",
    "    count = 0\n",
    "\n",
    "    with open(input_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                dt = parse(line, fuzzy=True)\n",
    "                iso_date = dt.strftime('%Y-%m-%d')\n",
    "                if dt.weekday() == target_weekday:\n",
    "                    count += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        outfile.write(str(count))\n",
    "\n",
    "# count_day_occurrences('/data/dates.txt', '/data/dates-wednesdays.txt', 'Wednesday')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A4 sort some json files\n",
    "\n",
    "def sort_json_file(input_file, output_file, sort_keys):\n",
    "    input_file = make_relative_path(input_file)\n",
    "    output_file = make_relative_path(output_file)\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    sorted_data = sorted(data, key=lambda x: [x[k] for k in sort_keys])\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A5 write recent file lines\n",
    "\n",
    "def write_recent_file_lines(file_dir, output_file, no_of_files, line_number, file_extension):\n",
    "    output_file = make_relative_path(output_file)\n",
    "    file_dir = make_relative_path(file_dir)\n",
    "    files = [os.path.join(file_dir, f) for f in os.listdir(file_dir) if f.endswith(file_extension)]\n",
    "    sorted_files = sorted(files, key=os.path.getmtime, reverse=True)[:no_of_files]\n",
    "    \n",
    "    with open(output_file, 'w') as outf:\n",
    "        for file in sorted_files:\n",
    "            try:\n",
    "                with open(file, 'r', encoding='utf-8') as inf:\n",
    "                    lines = inf.readlines()\n",
    "                    # Write the specified line if it exists; otherwise write an empty line\n",
    "                    if len(lines) >= line_number:\n",
    "                        outf.write(lines[line_number - 1].rstrip() + '\\n')\n",
    "                    else:\n",
    "                        outf.write('\\n')\n",
    "            except Exception:\n",
    "                outf.write('\\n')\n",
    "\n",
    "\n",
    "# write_recent_file_lines('/data/logs', '/data/logs-recent.txt', 10, 1, '.log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_json_file('/data/contacts.json', '/data/contacts-sorted.json', ['last_name', 'first_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have seen your kind time and time again\n",
      "I have seen your kind time and time again\n"
     ]
    }
   ],
   "source": [
    "# A6 markdown index creating\n",
    "\n",
    "\n",
    "\n",
    "def create_markdown_index(input_directory, output_index_file, occurrence=1):\n",
    "    input_directory = make_relative_path(input_directory)\n",
    "    output_index_file = make_relative_path(output_index_file)\n",
    "    \n",
    "    index = {}\n",
    "    \n",
    "    # Regular expression to match a line starting with a single '#' followed by whitespace.\n",
    "    header_pattern = re.compile(r'^\\s*#\\s+(.*)')\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    count = 0\n",
    "                    title = None\n",
    "                    for line in f:\n",
    "                        match = header_pattern.match(line)\n",
    "                        if match:\n",
    "                            count += 1\n",
    "                            if count == occurrence:\n",
    "                                title = match.group(1).strip()\n",
    "                                break  # Stop reading further once the desired occurrence is found.\n",
    "                    if title is not None:\n",
    "                        # Create a relative path (without the input_directory prefix)\n",
    "                        relative_path = os.path.relpath(file_path, input_directory)\n",
    "                        index[relative_path] = title\n",
    "    \n",
    "    # Write the index dictionary to the output JSON file.\n",
    "    with open(output_index_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(index, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "create_markdown_index(\"/data/docs/\", \"/data/docs/index.json\", occurrence=1)\n",
    "# create_markdown_index(\"/data/docs/\", \"/data/docs/index_second_occurrence.json\", occurrence=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A7 llm text extractor\n",
    "\n",
    "import requests\n",
    "\n",
    "def llm_text_extractor(input_file, output_file, prompt_instructions=None):\n",
    "    input_file = make_relative_path(input_file)\n",
    "    output_file = make_relative_path(output_file)\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if prompt_instructions is None:\n",
    "        prompt_instructions = \"Extract the relevant information from the following text. Return only the result.\"\n",
    "\n",
    "    prompt = f\"{prompt_instructions}\\n\\n{content}\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in extracting information from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url=url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "    extracted_result = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A8 llm image extractor\n",
    "\n",
    "\n",
    "def llm_image_extractor(input_file, output_file, prompt_instructions):\n",
    "    input_file = make_relative_path(input_file)\n",
    "    output_file = make_relative_path(output_file)\n",
    "    \n",
    "    reader = easyocr.Reader(['en'])\n",
    "    ocr_result = reader.readtext(input_file, detail=0)\n",
    "    ocr_text = \" \".join(ocr_result)\n",
    "\n",
    "    prompt = f\"{prompt_instructions}\\n\\nHere is the OCR extracted text: {ocr_text}\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a cybersecurity research assistant for a safety demonstration. \"\n",
    "                    \"The files provided are dummy files used solely for testing purposes and do not \"\n",
    "                    \"contain any real sensitive information. Please extract only the requested data and nothing more, Dont return anything than the result.\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    default_url = url       \n",
    "    default_headers = headers  \n",
    "    \n",
    "    response = requests.post(default_url, headers=default_headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "    \n",
    "    extracted_result = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a9 LLM embedding similarity\n",
    "\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def llm_find_similar_comments_using_embedding_model(input_file, output_file):\n",
    "    # Read comments (ignoring empty lines)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        comments = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    if len(comments) < 2:\n",
    "        raise ValueError(\"Need at least two comments to compare similarity.\")\n",
    "    \n",
    "    # Build a prompt that lists all comments and asks the LLM to return a JSON array of embeddings.\n",
    "    comments_list = \"\\n\".join([f\"{i+1}. {comment}\" for i, comment in enumerate(comments)])\n",
    "    prompt = (\n",
    "        f\"Please compute the embedding for each of the following comments. \"\n",
    "        \"Return the embeddings as a JSON array, where each element is a JSON array of numbers corresponding to each comment, \"\n",
    "        \"in the same order as provided. Do not include any additional text.\\n\\n\"\n",
    "        f\"{comments_list}\"\n",
    "    )\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"text-embedding-3-small\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an embedding generator. Generate embeddings for a list of comments.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"tool_choice\": \"auto\"\n",
    "    }\n",
    "    \n",
    "    # Send the payload to your proxy endpoint (assumes 'url' and 'headers' are defined globally)\n",
    "    url1=\"https://aiproxy.sanand.workers.dev/openai/v1/embeddings\"\n",
    "\n",
    "\n",
    "    response = requests.post(url1, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the embeddings from the LLM's response. Expecting a JSON array of embeddings.\n",
    "    embeddings_str = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    try:\n",
    "        embeddings = json.loads(embeddings_str)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"Failed to parse embeddings from the LLM response.\")\n",
    "    \n",
    "    if len(embeddings) != len(comments):\n",
    "        raise ValueError(\"The number of embeddings returned does not match the number of comments.\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Define cosine similarity function\n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    max_similarity = -1.0\n",
    "    best_pair = (None, None)\n",
    "    n = len(embeddings)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                best_pair = (comments[i], comments[j])\n",
    "    \n",
    "    # Write the most similar pair of comments to the output file, one per line.\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(best_pair[0] + \"\\n\" + best_pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A10 extract and excute sql query\n",
    "\n",
    "def llm_find_similar_comments_using_embedding_model(input_file, output_file):\n",
    "    input_file = make_relative_path(input_file)\n",
    "    output_file = make_relative_path(output_file)\n",
    "    # Read comments (ignoring empty lines)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        comments = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    if len(comments) < 2:\n",
    "        raise ValueError(\"Need at least two comments to compare similarity.\")\n",
    "    \n",
    "    # Build the payload using the \"input\" field with the list of comments.\n",
    "    payload = {\n",
    "        \"model\": \"text-embedding-3-small\",\n",
    "        \"input\": comments\n",
    "    }\n",
    "    \n",
    "    # Proxy endpoint for embeddings (ensure 'headers' is defined globally)\n",
    "    url1 = \"https://aiproxy.sanand.workers.dev/openai/v1/embeddings\"\n",
    "    \n",
    "    response = requests.post(url1, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # Expecting a response like: {\"data\": [ { \"embedding\": [...] }, ... ]}\n",
    "    if \"data\" not in data:\n",
    "        raise ValueError(\"No data returned in response.\")\n",
    "    \n",
    "    embeddings = [item.get(\"embedding\") for item in data[\"data\"]]\n",
    "    \n",
    "    if len(embeddings) != len(comments):\n",
    "        raise ValueError(\"The number of embeddings does not match the number of comments.\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Define cosine similarity function.\n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    max_similarity = -1.0\n",
    "    best_pair = (None, None)\n",
    "    n = len(embeddings)\n",
    "    \n",
    "    # Find the pair with the highest cosine similarity.\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                best_pair = (comments[i], comments[j])\n",
    "    \n",
    "    # Write the most similar pair of comments to the output file, one per line.\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(best_pair[0] + \"\\n\" + best_pair[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def execute_sql_query(db_file, sql_query, output_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(sql_query)\n",
    "    result = cursor.fetchall()\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    if len(result) == 1 and len(result[0]) == 1:\n",
    "        output = str(result[0][0])\n",
    "    else:\n",
    "        output = \"\\n\".join(str(row) for row in result)\n",
    "    \n",
    "    # Write the result to the output file.\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Calling Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list=[ {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"execute_generated_code\",\n",
    "            \"description\": \"Execute a given Python script with provided parameters.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Python code to execute.\"\n",
    "                    },\n",
    "                    \"arguments\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"description\": \"Key-value pairs of arguments to be passed to the script.\",\n",
    "                        \"additionalProperties\": True  \n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"code\"], \n",
    "                \"additionalProperties\": False \n",
    "            }\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_list = [ {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"execute_generated_code\",\n",
    "            \"description\": \"Execute a given Python script with provided parameters.The code should be provided as a string from the system.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Python code to execute. without comment ,make it as simple as possible\"\n",
    "                    },\n",
    "                    \"arguments\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"description\": \"Key-value pairs of arguments to be passed to the script.\",\n",
    "                        \"additionalProperties\": True  \n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"code\"], \n",
    "                \"additionalProperties\": False \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"count_day_occurrence\",\n",
    "            \"description\": \"Count occurrences of a specified day from dates in a file.Example prompt (The file /data/dates.txt contains a list of dates, one per line. Count the number of Wednesdays in the list, and write just the number to /data/dates-wednesdays.txt)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"input_file\": {\"type\": \"string\", \"description\": \"Input file with dates.\"},\n",
    "                    \"day\": {\"type\": \"string\", \"description\": \"Day to count.\"},\n",
    "                    \"output_file\": {\"type\": \"string\", \"description\": \"Output file for count.\"}\n",
    "                },\n",
    "                \"required\": [\"input_file\", \"day\", \"output_file\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"run_npx_package\",\n",
    "            \"description\": \"Run npx command with optional arg and output file.Example prompt(Format the contents of /data/format.md using prettier@3.4.2, updating the file in-place)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"package\": {\"type\": \"string\"},\n",
    "                    \"arg\": {\"type\": \"string\"},\n",
    "                    \"output_file\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"package\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"sort_json_file\",\n",
    "    \"description\": \"Sort JSON file by keys.example prompt(A4. Sort the array of contacts in /data/contacts.json by last_name, then first_name, and write the result to /data/contacts-sorted.json)\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"input_file\": { \"type\": \"string\" },\n",
    "        \"output_file\": { \"type\": \"string\" },\n",
    "        \"sort_keys\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": { \"type\": \"string\" }\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"input_file\", \"output_file\", \"sort_keys\"],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"write_recent_file_lines\",\n",
    "    \"description\": \"Extracts a specific line from the most recent files with a given extension in a directory and writes them to an output file.Example prompt (Write the first line of the 10 most recent .log file in /data/logs/ to /data/logs-recent.txt, most recent first)\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"file_dir\": { \"type\": \"string\" },\n",
    "        \"output_file\": { \"type\": \"string\" },\n",
    "        \"no_of_files\": { \"type\": \"integer\" ,\"description\": \"Number of recent files to process.\"},\n",
    "        \"line_number\": { \"type\": \"integer\" },\n",
    "        \"file_extension\": { \"type\": \"string\" }\n",
    "      },\n",
    "      \"required\": [\"file_dir\", \"output_file\", \"no_of_files\", \"line_number\", \"file_extension\"],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"run_download_from_script\",\n",
    "    \"description\": \"Downloads a Python script from the given URL, saves it locally using its original filename, and executes it using 'uv run'. If a user_email is provided, it is passed as the first argument, followed by any additional arguments(default value for email is 22f1001551@ds.study.iitm.ac.in).Example prompt(run https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py with user_value as the only argument. (NOTE: This will generate data files required for the next tasks.))\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"url\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"args\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": { \"type\": \"string\" }\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"url\"],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"create_markdown_index\",\n",
    "    \"description\": \"Recursively searches for Markdown (.md) files in a given input directory, extracts the nth occurrence of an H1 header (a line starting with '# '), and writes an index JSON file mapping each file's relative path to its extracted header.(Example prompt: Create an index of the first H1 header in each Markdown file in /data/docs/ and save it to /data/docs/index.json)\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"input_directory\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The root directory where Markdown files are located.\"\n",
    "        },\n",
    "        \"output_index_file\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The JSON file path where the index will be saved.\"\n",
    "        },\n",
    "        \"occurrence\": {\n",
    "          \"type\": \"integer\",\n",
    "          \"description\": \"The occurrence of the H1 header to extract (e.g., 1 for the first occurrence, 2 for the second, etc.).\",\n",
    "          \"default\": 1\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"input_directory\", \"output_index_file\"],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"llm_text_extractor\",\n",
    "    \"description\": \"Extracts information from a text using an LLM based on provided instructions.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"input_file\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"output_file\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"prompt_instructions\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Custom instructions for the LLM.The instructions must tell the llm to only should the results and nothing else.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"input_file\",\n",
    "        \"output_file\",\n",
    "        \"prompt_instructions\"\n",
    "      ],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"llm_image_extractor\",\n",
    "    \"description\": \"Extracts a credit card number from an image using an LLM for a cybersecurity safety demonstration.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"input_file\": {\n",
    "          \"type\": \"string\",\n",
    "        },\n",
    "        \"output_file\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"prompt_instructions\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Custom instructions for the LLM, including context for a cybersecurity safety demonstration.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"input_file\",\n",
    "        \"output_file\",\n",
    "        \"prompt_instructions\"\n",
    "      ],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"llm_find_similar_comments_using_embedding_model\",\n",
    "    \"description\": \"Finds the most similar pair of comments from a list using an LLM-based text embedding model.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"input_file\": {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"output_file\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"input_file\",\n",
    "        \"output_file\"\n",
    "      ],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"execute_sql_query\",\n",
    "    \"description\": \"Executes a given SQL query on a SQLite database file and writes the result to an output file.The LLM will generate the query\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"db_file\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Path to the SQLite database file.\"\n",
    "        },\n",
    "        \"sql_query\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The SQL query to execute.\"\n",
    "        },\n",
    "        \"output_file\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Path where the query result will be written.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"db_file\",\n",
    "        \"sql_query\",\n",
    "        \"output_file\"\n",
    "      ],\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "/data/comments.txt contains a list of comments, one per line. Using embeddings, find the most similar pair of comments and write them to /data/comments-similar.txt, one per lin\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url=url,\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"tools\": function_list,\n",
    "        \"tool_choice\": \"auto\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-B0tqHIKcBjwYvkzdznHATAVKrAxZL',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1739554357,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'tool_calls': [{'id': 'call_MLx2hMp0GdWfurSIHjj2V6CL',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'llm_find_similar_comments_using_embedding_model',\n",
       "       'arguments': '{\"input_file\":\"/data/comments.txt\",\"output_file\":\"/data/comments-similar.txt\"}'}}],\n",
       "    'refusal': None},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 1036,\n",
       "  'completion_tokens': 37,\n",
       "  'total_tokens': 1073,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_bd83329f63',\n",
       " 'monthlyCost': 0.165714,\n",
       " 'cost': 0.00333,\n",
       " 'monthlyRequests': 64}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_file': '/data/comments.txt', 'output_file': '/data/comments-similar.txt'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "arguments_str = response.json()['choices'][0]['message']['tool_calls'][0]['function']['arguments']\n",
    "parsed_arguments = json.loads(arguments_str)\n",
    "\n",
    "print(parsed_arguments)\n",
    "function_name=response.json()['choices'][0]['message']['tool_calls'][0]['function']['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llm_find_similar_comments_using_embedding_model'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name == \"execute_generated_code\":\n",
    "    code=parsed_arguments['code']\n",
    "    arguments = parsed_arguments.get('arguments', None)\n",
    "    output=execute_generated_code(code, arguments)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name==\"run_npx_package\":\n",
    "    package=parsed_arguments['package']\n",
    "    arg=parsed_arguments.get('arg', None)\n",
    "    run_npx_package(package, arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name==\"count_day_occurrence\":\n",
    "    input_file=parsed_arguments['input_file']\n",
    "    output_file=parsed_arguments['output_file']\n",
    "    day=parsed_arguments['day']\n",
    "    count_day_occurrences(input_file, output_file, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have seen you kind time and time again\n",
      "I have seen you kind time and time again\n"
     ]
    }
   ],
   "source": [
    "if function_name==\"sort_json_file\":\n",
    "    input_file=parsed_arguments['input_file']\n",
    "    output_file=parsed_arguments['output_file']\n",
    "    sort_keys=parsed_arguments['sort_keys']\n",
    "    sort_json_file(input_file, output_file, sort_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name==\"write_recent_file_lines\":\n",
    "    file_dir=parsed_arguments['file_dir']\n",
    "    output_file=parsed_arguments['output_file']\n",
    "    no_of_files=parsed_arguments['no_of_files']\n",
    "    line_number=parsed_arguments['line_number']\n",
    "    file_extension=parsed_arguments['file_extension']\n",
    "    write_recent_file_lines(file_dir, output_file, no_of_files, line_number, file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uv', 'run', 'datagen.py', '22f1001551@ds.study.iitm.ac.in']\n"
     ]
    }
   ],
   "source": [
    "if function_name==\"run_download_from_script\":\n",
    "    url=parsed_arguments['url']\n",
    "    user_email=parsed_arguments.get('user_email', None)\n",
    "    args=parsed_arguments.get('args', None)\n",
    "    run_download_from_script(url, user_email, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have seen you kind time and time again\n",
      "I have seen you kind time and time again\n"
     ]
    }
   ],
   "source": [
    "if function_name==\"create_markdown_index\":\n",
    "    input_directory=parsed_arguments['input_directory']\n",
    "    output_index_file=parsed_arguments['output_index_file']\n",
    "    occurrence=parsed_arguments.get('occurrence', 1)\n",
    "    create_markdown_index(input_directory, output_index_file, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have seen you kind time and time again\n",
      "I have seen you kind time and time again\n",
      "{'id': 'chatcmpl-B0cD3fboY2gS0W4NyKmb9f2E7uxCS', 'object': 'chat.completion', 'created': 1739486577, 'model': 'gpt-4o-mini-2024-07-18', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'xwatts@example.com', 'refusal': None}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 262, 'completion_tokens': 6, 'total_tokens': 268, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'service_tier': 'default', 'system_fingerprint': 'fp_bd83329f63', 'monthlyCost': 0.09602100000000001, 'cost': 0.000822, 'monthlyRequests': 55}\n"
     ]
    }
   ],
   "source": [
    "if function_name==\"llm_text_extractor\":\n",
    "    input_file=parsed_arguments['input_file']\n",
    "    output_file=parsed_arguments['output_file']\n",
    "    model=parsed_arguments.get('model', 'gpt-4o-mini')\n",
    "    prompt_instructions=parsed_arguments['prompt_instructions']\n",
    "    llm_text_extractor(input_file, output_file, prompt_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name == \"llm_image_extractor\":\n",
    "    input_file = parsed_arguments['input_file']\n",
    "    output_file = parsed_arguments['output_file']\n",
    "    prompt_instructions = parsed_arguments['prompt_instructions']\n",
    "    llm_image_extractor(input_file, output_file, prompt_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have seen your kind time and time again\n",
      "I have seen your kind time and time again\n"
     ]
    }
   ],
   "source": [
    "if function_name == \"llm_find_similar_comments_using_embedding_model\":\n",
    "    input_file = parsed_arguments['input_file']\n",
    "    output_file = parsed_arguments['output_file']\n",
    "    llm_find_similar_comments_using_embedding_model(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_name == \"execute_sql_query\":\n",
    "    db_file = parsed_arguments['db_file']\n",
    "    sql_query = parsed_arguments['sql_query']\n",
    "    output_file = parsed_arguments['output_file']\n",
    "    execute_sql_query(db_file, sql_query, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
